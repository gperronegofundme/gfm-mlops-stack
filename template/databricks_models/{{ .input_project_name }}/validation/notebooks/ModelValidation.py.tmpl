import argparse
import os
import tempfile
import traceback
import mlflow
from mlflow.tracking.client import MlflowClient
from pyspark.sql import SparkSession

class ModelValidation:
    def __init__(self, args):
        self.experiment_name = args.experiment_name
        self.run_mode = args.run_mode
        self.enable_baseline_comparison = args.enable_baseline_comparison.lower() == "true"
        self.validation_input = args.validation_input
        self.model_type = args.model_type
        self.targets = args.targets
        self.custom_metrics_loader_function = args.custom_metrics_loader_function
        self.validation_thresholds_loader_function = args.validation_thresholds_loader_function
        self.evaluator_config_loader_function = args.evaluator_config_loader_function
        self.model_name = args.model_name
        self.model_version = args.model_version
        self.registry_uri = args.registry_uri

        self.client = MlflowClient(registry_uri=self.registry_uri) if self.registry_uri else MlflowClient()
        mlflow.set_registry_uri(self.registry_uri) if self.registry_uri else None

    def setup_experiment(self):
        mlflow.set_experiment(self.experiment_name)

    def get_data(self):
        spark = SparkSession.builder.appName("ModelValidation").getOrCreate()
        data = spark.sql(self.validation_input)
        return data

    def load_functions(self):
        custom_metrics_loader = getattr(importlib.import_module("validation"), self.custom_metrics_loader_function)
        validation_thresholds_loader = getattr(importlib.import_module("validation"), self.validation_thresholds_loader_function)
        evaluator_config_loader = getattr(importlib.import_module("validation"), self.evaluator_config_loader_function)

        custom_metrics = custom_metrics_loader()
        validation_thresholds = validation_thresholds_loader()
        evaluator_config = evaluator_config_loader()

        return custom_metrics, validation_thresholds, evaluator_config

    def get_model_info(self):
        if not self.model_version:
            self.model_version = dbutils.jobs.taskValues.get("Train", "model_version", debugValue="")
        if not self.model_name:
            self.model_name = dbutils.jobs.taskValues.get("Train", "model_name", debugValue="")
        if not self.model_name or not self.model_version:
            raise ValueError("Model name and version must be specified.")

        model_uri = f"models:/{self.model_name}/{self.model_version}"
        baseline_model_uri = f"models:/{self.model_name}/Production" if not self.enable_baseline_comparison else f"models:/{self.model_name}@champion"
        return model_uri, baseline_model_uri

    def log_artifacts(self, validation_thresholds, eval_result, tmp_dir, custom_metrics=None):
        validation_thresholds_file = os.path.join(tmp_dir, "validation_thresholds.txt")
        with open(validation_thresholds_file, "w") as f:
            for metric_name in validation_thresholds:
                f.write(f"{metric_name:30}  {validation_thresholds[metric_name]}\n")
        mlflow.log_artifact(validation_thresholds_file)

        metrics_file = os.path.join(tmp_dir, "metrics.txt")
        with open(metrics_file, "w") as f:
            f.write(f"{'metric_name':30}  {'candidate':30}  {'baseline'}\n")
            for metric in eval_result.metrics:
                candidate_metric_value = str(eval_result.metrics[metric])
                baseline_metric_value = "N/A"
                if metric in eval_result.baseline_model_metrics:
                    mlflow.log_metric(f"baseline_{metric}", eval_result.baseline_model_metrics[metric])
                    baseline_metric_value = str(eval_result.baseline_model_metrics[metric])
                f.write(f"{metric:30}  {candidate_metric_value:30}  {baseline_metric_value}\n")
        mlflow.log_artifact(metrics_file)

    def log_to_model_description(self, run, success):
        run_link = self.get_run_link(run.info)
        description = self.client.get_model_version(self.model_name, self.model_version).description
        status = "SUCCESS" if success else "FAILURE"
        if description:
            description += "\n\n---\n\n"
        description += f"Model Validation Status: {status}\nValidation Details: {run_link}"
        self.client.update_model_version(name=self.model_name, version=self.model_version, description=description)

    def get_run_link(self, run_info):
        return f"[Run](#mlflow/experiments/{run_info.experiment_id}/runs/{run_info.run_id})"

    def get_training_run(self):
        version = self.client.get_model_version(self.model_name, self.model_version)
        return mlflow.get_run(run_id=version.run_id)

    def validate_model(self):
        if self.run_mode == "disabled":
            print("Model validation is in DISABLED mode. Exiting.")
            return

        data = self.get_data()
        custom_metrics, validation_thresholds, evaluator_config = self.load_functions()

        training_run = self.get_training_run()
        run_name = f"{training_run.info.run_name}-validation" if training_run else None
        description = f"Model Training Details: {self.get_run_link(training_run.info)}" if training_run else None

        with mlflow.start_run(run_name=run_name, description=description) as run, tempfile.TemporaryDirectory() as tmp_dir:
            model_uri, baseline_model_uri = self.get_model_info()
            try:
                eval_result = mlflow.evaluate(
                    model=model_uri,
                    data=data,
                    targets=self.targets,
                    model_type=self.model_type,
                    evaluators="default",
                    validation_thresholds=validation_thresholds,
                    custom_metrics=custom_metrics,
                    baseline_model=baseline_model_uri if self.enable_baseline_comparison else None,
                    evaluator_config=evaluator_config,
                )

                self.log_artifacts(validation_thresholds, eval_result, tmp_dir)
                self.log_to_model_description(run, True)

                if "{{ .input_include_models_in_unity_catalog }}" == "yes":
                    print("Validation checks passed. Assigning 'challenger' alias to model version.")
                    self.client.set_registered_model_alias(self.model_name, "challenger", self.model_version)

            except Exception as err:
                self.log_to_model_description(run, False)
                error_file = os.path.join(tmp_dir, "error.txt")
                with open(error_file, "w") as f:
                    f.write(f"Validation failed : {err}\n")
                    f.write(traceback.format_exc())
                mlflow.log_artifact(error_file)
                if not self.run_mode == "dry_run":
                    raise err
                else:
                    print("Model validation failed in DRY_RUN. It will not block model deployment.")

def parse_args():
    parser = argparse.ArgumentParser(description="Model Validation Script")
    parser.add_argument("--experiment_name", required=True, help="MLflow experiment name")
    parser.add_argument("--run_mode", required=True, choices=["disabled", "dry_run", "enabled"], help="Run mode")
    parser.add_argument("--enable_baseline_comparison", required=True, choices=["true", "false"], help="Enable baseline comparison")
    parser.add_argument("--validation_input", required=True, help="Validation input SQL query")
    parser.add_argument("--model_type", required=True, help="Model type (e.g., regressor, classifier)")
    parser.add_argument("--targets", required=True, help="Target column name in the dataset")
    parser.add_argument("--custom_metrics_loader_function", required=True, help="Custom metrics loader function")
    parser.add_argument("--validation_thresholds_loader_function", required=True, help="Validation thresholds loader function")
    parser.add_argument("--evaluator_config_loader_function", required=True, help="Evaluator config loader function")
    parser.add_argument("--model_name", required=True, help="Model name")
    parser.add_argument("--model_version", required=True, help="Candidate model version")
    parser.add_argument("--registry_uri", required=False, default=None, help="MLflow registry URI")
    
    return parser.parse_args()

def main():
    args = parse_args()
    model_validation = ModelValidation(args)
    model_validation.setup_experiment()
    model_validation.validate_model()

if __name__ == "__main__":
    main()